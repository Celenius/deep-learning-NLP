{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Very basic NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "smyxeRFvyntF"
      },
      "source": [
        "# Artificial network can only processes tensor (multidimensional matrix)\n",
        "# So in order to process text, we need to transform it into tensor\n",
        "# We can do so by : \n",
        "#   - one-hot-encoding the text (sparse matrix, moslty full of 0, higher dimmension, 1 unique word = 1 additional dimension, hardcoded)\n",
        "#   - Word embeding (dense matrix, fewer dimension - often 256/512/1024, learned from data)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJZDkVfkzVL6"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSBHdIjVzak4"
      },
      "source": [
        "# Exemple from scratch\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1\n",
        "\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) +1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE-l28OE0qeR"
      },
      "source": [
        "# Exemple using Keras API\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwQuNwAw1cUf"
      },
      "source": [
        "tokens = Tokenizer(num_words=1000) # Take only the 1000 most common words\n",
        "tokens.fit_on_texts(samples) # Build the word index"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kGehVbQ1xn_"
      },
      "source": [
        "sequences = tokens.texts_to_sequences(samples) # Turns string into list of integer indices"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvN91joD2xvw"
      },
      "source": [
        "one_hot_results = tokens.texts_to_matrix(sample, mode='binary')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZaZlANH25gg",
        "outputId": "c4ddf23f-73f5-43d8-d7df-7fdf401c094d"
      },
      "source": [
        "word_index = tokens.word_index\n",
        "print(\"found\", len(word_index), \"unique tokens\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 9 unique tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFCUZN1m3mSh"
      },
      "source": [
        "# If there is too much unique words and memory is an issue, we can \"hash\" word (but hash collision may occurs)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEKY7l4U3xb_"
      },
      "source": [
        "# Using Embeding transformation\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EDH2iO6FXzK"
      },
      "source": [
        "# Map integer to  vector\n",
        "# Word index => Ebeding layer => Word vector\n",
        "\n",
        "embeding_layer = Embedding(1000, 64)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OAqY_RFmTl1",
        "outputId": "63f07dad-75c3-45fb-af56-19795d3560d1"
      },
      "source": [
        "embeding_layer"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.embeddings.Embedding at 0x7fa7ef733150>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}